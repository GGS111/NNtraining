{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "frank-norway",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'z_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f0869cf84088>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mz_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils_base_14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mz_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils_7343_01\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0monnx1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'z_utils'"
     ]
    }
   ],
   "source": [
    "import matplotlib.mlab as mat\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "from z_utils.utils_base_14 import *\n",
    "from z_utils.utils_7343_01 import *\n",
    "from onnx1 import *\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#import sys\n",
    "import os\n",
    "#import pathlib\n",
    "import time\n",
    "from z_utils.utils_base_14 import *\n",
    "from z_utils.utils_mat_torch_003 import *\n",
    "from z_utils.utils_7343_01 import *\n",
    "from zz.Model_SRR_deep_YUV import model_SRR_03_1cannal_skotch\n",
    "from zz.sketch2color_00 import N_sketch_2_color_00\n",
    " \n",
    "from skimage.morphology import skeletonize, thin \n",
    "from enum import Enum\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from zz.gan_struct_04a import Layer_09, quasy_conv_00,quasy_conv_01_soft, conv_layer_downsample_01,Layer_01\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Flatten\n",
    "from torch.nn import LeakyReLU\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Softmax  \n",
    "from torch.nn import MaxPool2d,AvgPool2d\n",
    "from enum import Enum\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import BatchNorm2d,BatchNorm1d\n",
    "from torch import nn\n",
    "#####################################################################3\n",
    "from zz.layers import Layer\n",
    "from zz.layers.Input import Input\n",
    "from zz.layers.Lambda import Lambda\n",
    "from zz.layers.Reshape import Reshape\n",
    "from zz.layers.Flatten import Flatten\n",
    "from zz.utils.torchsummary import summary as _summary\n",
    "from zz.utils.WrappedDataLoader import WrappedDataLoader\n",
    "from zz.utils.History import History\n",
    "from zz.utils.Regularizer import Regularizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from zz.Convolution_uno_01   import conv_layer_universal_uno_04,conv_layer_universal_uno_05\n",
    "from zz.layers.Layer_01 import Layer_01\n",
    "from zz.gan_struct_04a import Layer_06\n",
    "################################################3\n",
    "################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expired-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################3\n",
    "class conv_simple_features_00(Layer_06):\n",
    "    def __init__(self,  device = None, L1 = 0., L2 = 0.,show=0):\n",
    "        super(conv_simple_features_00, self).__init__()\n",
    "        self.show = show\n",
    "        if (device is not None):\n",
    "            self.device = device if (not isinstance(device, str)) else torch.device(device)\n",
    "        else:\n",
    "            self.device = device if (device is not None) else \\\n",
    "                    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                \n",
    "        self.L1=L1\n",
    "        self.L2=L2\n",
    "        self.regularizer = Regularizer(L1, L2)\n",
    "        _layer_conv_31 = Conv2d(1,4, kernel_size=(5, 5),\n",
    "                            stride=(4, 4), padding = (2, 2), padding_mode = 'zeros', bias = True)\n",
    "        self.add_module('conv00', _layer_conv_31) \n",
    "        _layer_activation_1 = LeakyReLU(0.05) \n",
    "        self.add_module('activation_LeakyReLU', _layer_activation_1)\n",
    "        _layer_conv_32 = Conv2d(4,16, kernel_size=(5,5),\n",
    "                            stride=(4, 4), padding = (2, 2), padding_mode = 'zeros', bias = True)\n",
    "        \n",
    "        self.add_module('conv01', _layer_conv_32) \n",
    "        _layer_conv_33 = Conv2d(16,22, kernel_size=(3,3),\n",
    "                            stride=(1, 1), padding = (1, 1), padding_mode = 'zeros', bias = True)\n",
    "          \n",
    "        \n",
    "        self.add_module('conv02', _layer_conv_33) \n",
    "        _layer_pooling_1 = MaxPool2d(kernel_size=(2, 2))  \n",
    "        self.add_module('Pool_00', _layer_pooling_1) \n",
    "        self.add_module('fltn_1', Flatten( ))\n",
    "\n",
    "        self.to(self.device)\n",
    "        self.reset_parameters()\n",
    " \n",
    "    def forward(self, scatch0):\n",
    "        im_01_dwnsmpl=self.conv00(scatch0)\n",
    "        im_01_dwnsmpl=self.activation_LeakyReLU(im_01_dwnsmpl)\n",
    "        if self.show:\n",
    "            print('im_01_dwnsmpl',im_01_dwnsmpl.shape)    \n",
    "        im_02_dwnsmpl=self.conv01(im_01_dwnsmpl)\n",
    "        im_02_dwnsmpl=self.activation_LeakyReLU(im_02_dwnsmpl)\n",
    "        if self.show:\n",
    "            print('im_02_dwnsmpl',im_02_dwnsmpl.shape)    \n",
    "        im_03_dwnsmpl=self.conv02(im_02_dwnsmpl)\n",
    "        im_03_dwnsmpl=self.Pool_00(im_03_dwnsmpl) \n",
    "        im_03_dwnsmpl=self.activation_LeakyReLU(im_03_dwnsmpl)\n",
    "        if self.show:\n",
    "            print('im_03_dwnsmpl',im_03_dwnsmpl.shape)    \n",
    "        vect_00=self.fltn_1(im_03_dwnsmpl)\n",
    "        if self.show:\n",
    "            print('vect_00',vect_00.shape)   \n",
    "        return vect_00\n",
    "################################################3\n",
    "class fully_connect_modul_264(Layer_06):\n",
    "    def __init__(self,  device = None, L1 = 0., L2 = 0., numclasses=9, show=0):\n",
    "        super(fully_connect_modul_264, self).__init__()\n",
    "        self.show = show\n",
    "        if (device is not None):\n",
    "            self.device = device if (not isinstance(device, str)) else torch.device(device)\n",
    "        else:\n",
    "            self.device = device if (device is not None) else \\\n",
    "                    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.numclasses =numclasses        \n",
    "        self.L1=L1\n",
    "        self.L2=L2\n",
    "        self.regularizer = Regularizer(L1, L2)\n",
    "        _layer_activation_1 = LeakyReLU(0.05) \n",
    "        self.add_module('activation_LeakyReLU', _layer_activation_1)\n",
    "        _layer_D01 = Linear(352, 256, bias = True)\n",
    "        self.add_module('D01', _layer_D01)\n",
    "        _layer_Dropout01 = Dropout(0.5)\n",
    "        _layer_batch_norm_3 = BatchNorm1d(256)\n",
    "        self.add_module('Dropout01', _layer_Dropout01) \n",
    "        self.add_module('layer_batch_norm',  _layer_batch_norm_3) \n",
    "        _layer_D02 = Linear(256, 128, bias = True)\n",
    "        self.add_module('D02', _layer_D02)\n",
    "        _layer_D03 = Linear(128,  self.numclasses, bias = True)\n",
    "        self.add_module('D03', _layer_D03)\n",
    "       \n",
    "        \n",
    "        _layer_SfTMax = Softmax(dim = -1)\n",
    "        self.add_module('SfTMax', _layer_SfTMax)  \n",
    "        _layer_Sgmd = Sigmoid()\n",
    "        self.add_module('Sgmd', _layer_Sgmd)  \n",
    "        #########################\n",
    "        self.to(self.device)\n",
    "        self.reset_parameters()\n",
    " \n",
    "    def forward(self, vect_00):\n",
    "        vect_01=self.D01(vect_00)        \n",
    "        vect_01=self.Dropout01(vect_01)\n",
    "        vect_01=self.activation_LeakyReLU(vect_01)\n",
    "        if self.show:\n",
    "            print('vect_01',vect_01.shape) \n",
    "        vect_01=self.layer_batch_norm(vect_01)\n",
    "        if self.show:\n",
    "            print('vect_01 layer_batch_norm',vect_01.shape) \n",
    "        vect_02=self.D02(vect_01) \n",
    "        vect_02=self.activation_LeakyReLU(vect_02)\n",
    "        if self.show:\n",
    "            print('vect_02',vect_02.shape)    \n",
    "        vect_03=self.D03(vect_02) \n",
    "        vect_03=self.SfTMax(vect_03) \n",
    "         \n",
    "        if self.show:\n",
    "            print('vect_03',vect_03.shape)         \n",
    "        return vect_03\n",
    "################################################3\n",
    "class fully_connect_modul_265(Layer_06):\n",
    "    def __init__(self,  device = None, L1 = 0., L2 = 0.,   show=0):\n",
    "        super(fully_connect_modul_265, self).__init__()\n",
    "        self.show = show\n",
    "        if (device is not None):\n",
    "            self.device = device if (not isinstance(device, str)) else torch.device(device)\n",
    "        else:\n",
    "            self.device = device if (device is not None) else \\\n",
    "                    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "             \n",
    "        self.L1=L1\n",
    "        self.L2=L2\n",
    "        self.regularizer = Regularizer(L1, L2)\n",
    "        _layer_activation_1 = LeakyReLU(0.05) \n",
    "        self.add_module('activation_LeakyReLU', _layer_activation_1)\n",
    "        _layer_D01 = Linear(352, 256, bias = True)\n",
    "        self.add_module('D01', _layer_D01)\n",
    "        _layer_Dropout01 = Dropout(0.5)\n",
    "        _layer_batch_norm_3 = BatchNorm1d(256)\n",
    "        self.add_module('Dropout01', _layer_Dropout01) \n",
    "        self.add_module('layer_batch_norm',  _layer_batch_norm_3) \n",
    "        _layer_D02 = Linear(256, 128, bias = True)\n",
    "        self.add_module('D02', _layer_D02)\n",
    "        _layer_D03 = Linear(128,  64, bias = True)\n",
    "        self.add_module('D03', _layer_D03)\n",
    "       \n",
    "        \n",
    "        \n",
    "        _layer_Sgmd = Sigmoid()\n",
    "        self.add_module('Sgmd', _layer_Sgmd)  \n",
    "        #########################\n",
    "        self.to(self.device)\n",
    "        self.reset_parameters()\n",
    " \n",
    "    def forward(self, vect_00):\n",
    "        vect_01=self.D01(vect_00)        \n",
    "        vect_01=self.Dropout01(vect_01)\n",
    "        vect_01=self.activation_LeakyReLU(vect_01)\n",
    "        if self.show:\n",
    "            print('vect_01',vect_01.shape) \n",
    "        vect_01=self.layer_batch_norm(vect_01)\n",
    "        if self.show:\n",
    "            print('vect_01 layer_batch_norm',vect_01.shape) \n",
    "        vect_02=self.D02(vect_01) \n",
    "        vect_02=self.activation_LeakyReLU(vect_02)\n",
    "        if self.show:\n",
    "            print('vect_02',vect_02.shape)    \n",
    "        vect_03=self.D03(vect_02) \n",
    "        \n",
    "         \n",
    "        if self.show:\n",
    "            print('vect_03',vect_03.shape)         \n",
    "        return vect_03\n",
    "\n",
    "#№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№\n",
    "class TL_002_mehanit(Layer_06):\n",
    "    def __init__(self, imageSize,  last_activate, L1 = 0., L2 = 0.,device = None,numclasses=10,show=0 ):\n",
    "        super(TL_002_mehanit, self).__init__( (imageSize[0],imageSize[1],1)  )    \n",
    "\n",
    "        #self.class_name = str(self.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "        self.class_name = self.__class__.__name__\n",
    "        self.last_activate = last_activate\n",
    "        self.cannal_in= imageSize[2]\n",
    "         \n",
    "        self.imageSize = imageSize\n",
    "        self.regularizer = Regularizer(L1, L2)\n",
    "        self.show=show\n",
    "        self.L1=L1\n",
    "        self.L2=L2\n",
    "        self.numclasses=numclasses \n",
    "        self.criterion_tml = torch.nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "        if (device is not None):\n",
    "            self.device = device if (not isinstance(device, str)) else torch.device(device)\n",
    "        else:\n",
    "            self.device = device if (device is not None) else \\\n",
    "                    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        ##############3\n",
    "        self.conv2Dfeatures=conv_simple_features_00(device,L1 ,L2,show) \n",
    "        self.fully_connect_modul_0=fully_connect_modul_264(device,L1 ,L2,numclasses,show) \n",
    "        self.fully_connect_modul_TL=fully_connect_modul_265(device,L1 ,L2,show) \n",
    "         #######################\n",
    "        _layer_activation_1 = LeakyReLU(0.05) \n",
    "        self.add_module('activation_LeakyReLU', _layer_activation_1)\n",
    "        _layer_D01 = Linear(352, 256, bias = True)\n",
    "        self.add_module('D01', _layer_D01)\n",
    "        _layer_Dropout01 = Dropout(0.5)\n",
    "        _layer_batch_norm_3 = BatchNorm1d(256)\n",
    "        self.add_module('Dropout01', _layer_Dropout01) \n",
    "        self.add_module('layer_batch_norm',  _layer_batch_norm_3) \n",
    "        _layer_D02 = Linear(256, 128, bias = True)\n",
    "        self.add_module('D02', _layer_D02)\n",
    "        _layer_D03 = Linear(128,  self.numclasses, bias = True)\n",
    "        self.add_module('D03', _layer_D03)\n",
    "        #########################\n",
    "        \n",
    "        _layer_SfTMax = Softmax(dim = -1)\n",
    "        self.add_module('SfTMax', _layer_SfTMax)  \n",
    "        _layer_Sgmd = Sigmoid()\n",
    "        self.add_module('Sgmd', _layer_Sgmd)  \n",
    " \n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    #####################################################\n",
    "    def forward(self, scatch):\n",
    "        # skotch_N_global, Ref_global,sketch_ref\n",
    "        class _type_input(Enum):\n",
    "            is_torch_tensor = 0\n",
    "            is_numpy = 1\n",
    "            is_list = 2\n",
    "        \n",
    "           \n",
    "        x_input = (scatch , scatch)\n",
    "        \n",
    "        _t_input = []\n",
    "        _x_input = []\n",
    "        for x in x_input:\n",
    "            if isinstance(x, (torch.Tensor)):\n",
    "                _t_input.append(_type_input.is_torch_tensor)\n",
    "                _x_input.append(x)\n",
    "            elif isinstance(x, (np.ndarray)):\n",
    "                _t_input.append(_type_input.is_numpy)\n",
    "                _x_input.append(torch.FloatTensor(x).to(self.device))\n",
    "            elif isinstance(x, (list, tuple)):\n",
    "                _t_input.append(_type_input.is_list)\n",
    "                _x_input.append(torch.FloatTensor(x).to(self.device))\n",
    "            else:\n",
    "                raise Exception('Invalid type input')\n",
    "\n",
    "        _x_input = tuple(_x_input)\n",
    "        _t_input = tuple(_t_input)\n",
    "\n",
    "        scatch = self._contiguous(_x_input[0])\n",
    "        scatch1 = self._contiguous(_x_input[1])\n",
    "         \n",
    "         \n",
    "        ##############\n",
    "        \n",
    "            \n",
    "        \n",
    "        _layer_permut_channelfirst = Lambda(lambda x:  x.permute((0, 3, 1, 2)))\n",
    "        _layer_permut_channellast = Lambda(lambda x:  x.permute((0, 2, 3, 1)))\n",
    "\n",
    "        scatch0 = _layer_permut_channelfirst(scatch)\n",
    "        vect_00=self.conv2Dfeatures(scatch0)\n",
    "        vect_03=self.fully_connect_modul_0(vect_00)    \n",
    "        ################################# \n",
    "         \n",
    "        ######################################\n",
    "        x = vect_03\n",
    "        x = self._contiguous(x)\n",
    "        x1=x\n",
    "\n",
    "        ###################    \n",
    "\n",
    "        if _type_input.is_torch_tensor in _t_input:\n",
    "            pass\n",
    "        elif _type_input.is_numpy in _t_input:\n",
    "            if (self.device.type == \"cuda\"):\n",
    "                x = x.cpu().detach().numpy()\n",
    "            else:\n",
    "                x = x.detach().numpy()\n",
    "        else:\n",
    "            if (self.device.type == \"cuda\"):\n",
    "                x = x.cpu().detach().numpy().tolist()\n",
    "            else:\n",
    "                x = x.detach().numpy().tolist()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x1\n",
    "     \n",
    "    def _get_regularizer(self):\n",
    "        return self.regularizer\n",
    "#################################################################\n",
    "###################################################################################################\n",
    "    def loss_batch_02(self,dsrmn_model,  x,   opt=None):\n",
    "#            def cross_entropy(pred, soft_targets):\n",
    "#                return -torch.log(torch.mean(torch.sum(soft_targets * pred, 1)))\n",
    "            #logsoftmax = nn.LogSoftmax()\n",
    "            #return torch.pow(1 - torch.mean(torch.sum(soft_targets * pred, 1)), 2)\n",
    "            #return torch.mean(torch.sum(- soft_targets * torch.norm(pred, p=2,dim=1), 1))\n",
    "            #return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n",
    "#            def l2_norm_diff(pred, soft_targets):\n",
    "#                return  torch.sqrt(torch.mean(torch.sum((soft_targets - pred )**2,-1)))\n",
    "            #logsoftmax = nn.LogSoftmax()\n",
    "            #return torch.pow(1 - torch.mean(torch.sum(soft_targets * pred, 1)), 2)\n",
    "            #return torch.mean(torch.sum(- soft_targets * torch.norm(pred, p=2,dim=1), 1))\n",
    "            #return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n",
    "        \n",
    "        xb=(x[0],x[0]) \n",
    "        pred_ancor_0,_ = self.forward_eshar_00(*xb)\n",
    "        xb=(x[1],x[1])\n",
    "        pred_positive_0,_ = self.forward_eshar_00(*xb)\n",
    "        xb=(x[2],x[2])\n",
    "        pred_neg_0,_ = self.forward_eshar_00(*xb)\n",
    "        #print('self._criterion',self._criterion)\n",
    "        loss_0=self.criterion_tml (pred_ancor_0,pred_positive_0,pred_neg_0)\n",
    "        loss_1=0#self.criterion_tml (pred_ancor_1,pred_positive_1,pred_neg_1)\n",
    "            \n",
    "            \n",
    "            \n",
    "             \n",
    "            \n",
    "             \n",
    "         \n",
    "        loss =1.0*loss_0+0.9*loss_1 \n",
    "        \n",
    "         \n",
    "        #print(' loss', loss) #,end='')    \n",
    "       \n",
    "        #print(self.loss_vgg_1_bw(pred0, yb),self._criterion(pred0, yb),self.MSELoss( dscrm_tenzor,0*dscrm_tenzor))\n",
    "         \n",
    "        \n",
    "        #loss+=  2.1*loss_mse \n",
    "        #loss = loss_func(pred0, yb)\n",
    "        #loss = cross_entropy(pred, yb)\n",
    "\n",
    "        #del(pred_ancor_0,pred_ancor_1,pred_positive_0,pred_positive_1,pred_neg_0,pred_neg_1) \n",
    "\n",
    "        #_, predicted = torch.max(pred.data, dim = 1)\n",
    "        #_, ind_target = torch.max(yb, dim = 1)\n",
    "        #correct = (predicted == ind_target).sum().item()\n",
    "        #acc = correct / len(yb) #.size(0)\n",
    "\n",
    "        _regularizer = self._get_regularizer()\n",
    "\n",
    "        reg_loss = 0\n",
    "        for param in self.parameters():\n",
    "            reg_loss += _regularizer(param)\n",
    "\n",
    "        loss += reg_loss\n",
    "\n",
    "        if (opt is not None) :\n",
    "            with torch.no_grad():\n",
    "\n",
    "                opt.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                opt.step()\n",
    "\n",
    "        self.count+=1\n",
    "        if self.count  %3==0:\n",
    "            print(\"*\", end='')\n",
    "\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        del loss\n",
    "        del reg_loss\n",
    "        \n",
    "        return loss_item, 1#, acc\n",
    "################################################################\n",
    "    #####################################################\n",
    "    def forward_eshar_00(self, scatch, im_wire):\n",
    "        # skotch_N_global, Ref_global,sketch_ref\n",
    "        class _type_input(Enum):\n",
    "            is_torch_tensor = 0\n",
    "            is_numpy = 1\n",
    "            is_list = 2\n",
    "        \n",
    "           \n",
    "        x_input = (scatch , im_wire)\n",
    "        \n",
    "        _t_input = []\n",
    "        _x_input = []\n",
    "        for x in x_input:\n",
    "            if isinstance(x, (torch.Tensor)):\n",
    "                _t_input.append(_type_input.is_torch_tensor)\n",
    "                _x_input.append(x)\n",
    "            elif isinstance(x, (np.ndarray)):\n",
    "                _t_input.append(_type_input.is_numpy)\n",
    "                _x_input.append(torch.FloatTensor(x).to(self.device))\n",
    "            elif isinstance(x, (list, tuple)):\n",
    "                _t_input.append(_type_input.is_list)\n",
    "                _x_input.append(torch.FloatTensor(x).to(self.device))\n",
    "            else:\n",
    "                raise Exception('Invalid type input')\n",
    "\n",
    "        _x_input = tuple(_x_input)\n",
    "        _t_input = tuple(_t_input)\n",
    "\n",
    "        scatch = self._contiguous(_x_input[0])\n",
    "        im_wire = self._contiguous(_x_input[1])\n",
    "         \n",
    "         \n",
    "        ##############\n",
    "        \n",
    "            \n",
    "        \n",
    "        _layer_permut_channelfirst = Lambda(lambda x:  x.permute((0, 3, 1, 2)))\n",
    "        _layer_permut_channellast = Lambda(lambda x:  x.permute((0, 2, 3, 1)))\n",
    "\n",
    "        scatch0 = _layer_permut_channelfirst(scatch)\n",
    "        vect_00=self.conv2Dfeatures(scatch0)\n",
    "        vect_03=self.fully_connect_modul_TL(vect_00)    \n",
    "        ################################# \n",
    "         \n",
    "        ######################################\n",
    "        x = vect_03\n",
    "        x = self._contiguous(x)\n",
    "\n",
    "        ###################    \n",
    "\n",
    "        if _type_input.is_torch_tensor in _t_input:\n",
    "            pass\n",
    "        elif _type_input.is_numpy in _t_input:\n",
    "            if (self.device.type == \"cuda\"):\n",
    "                x = x.cpu().detach().numpy()\n",
    "            else:\n",
    "                x = x.detach().numpy()\n",
    "        else:\n",
    "            if (self.device.type == \"cuda\"):\n",
    "                x = x.cpu().detach().numpy().tolist()\n",
    "            else:\n",
    "                x = x.detach().numpy().tolist()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x,0\n",
    "     \n",
    "    def _get_regularizer(self):\n",
    "        return self.regularizer\n",
    "\n",
    "###################################################################\n",
    "    def loss_batch_01(self,dsrmn_model, xb, yb,   opt=None):\n",
    "#            def cross_entropy(pred, soft_targets):\n",
    "#                return -torch.log(torch.mean(torch.sum(soft_targets * pred, 1)))\n",
    "            #logsoftmax = nn.LogSoftmax()\n",
    "            #return torch.pow(1 - torch.mean(torch.sum(soft_targets * pred, 1)), 2)\n",
    "            #return torch.mean(torch.sum(- soft_targets * torch.norm(pred, p=2,dim=1), 1))\n",
    "            #return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n",
    "#            def l2_norm_diff(pred, soft_targets):\n",
    "#                return  torch.sqrt(torch.mean(torch.sum((soft_targets - pred )**2,-1)))\n",
    "            #logsoftmax = nn.LogSoftmax()\n",
    "            #return torch.pow(1 - torch.mean(torch.sum(soft_targets * pred, 1)), 2)\n",
    "            #return torch.mean(torch.sum(- soft_targets * torch.norm(pred, p=2,dim=1), 1))\n",
    "            #return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n",
    "\n",
    "        #print(xb[0].shape)\n",
    "        pred = self(*xb)\n",
    "        #print(pred)  \n",
    "        Positive=xb[0][0].numpy() \n",
    "        #ai_2(Positive[:,:,0])\n",
    "        #print(yb)        \n",
    "        #print('999999999999999999999999999999999')    \n",
    "\n",
    "        if isinstance(pred, tuple):\n",
    "            pred0 = pred[0]\n",
    "            del pred\n",
    "        else:\n",
    "            pred0 = pred\n",
    "        loss=0\n",
    "        \n",
    "        #loss_mse= self._criterion(pred0, yb) \n",
    "        #print(pred0.shape)\n",
    "        #print(yb.shape)\n",
    "        MSELoss=nn.MSELoss(reduction='mean')    \n",
    "        loss_mse= MSELoss(pred0, yb) \n",
    "         \n",
    "        \n",
    "        loss +=1.1*loss_mse \n",
    "       \n",
    "        #print(self.loss_vgg_1_bw(pred0, yb),self._criterion(pred0, yb),self.MSELoss( dscrm_tenzor,0*dscrm_tenzor))\n",
    "         \n",
    "        \n",
    "        #loss+=  2.1*loss_mse \n",
    "        #loss = loss_func(pred0, yb)\n",
    "        #loss = cross_entropy(pred, yb)\n",
    "\n",
    "        del pred0\n",
    "\n",
    "        #_, predicted = torch.max(pred.data, dim = 1)\n",
    "        #_, ind_target = torch.max(yb, dim = 1)\n",
    "        #correct = (predicted == ind_target).sum().item()\n",
    "        #acc = correct / len(yb) #.size(0)\n",
    "\n",
    "        _regularizer = self._get_regularizer()\n",
    "\n",
    "        reg_loss = 0\n",
    "        for param in self.parameters():\n",
    "            reg_loss += _regularizer(param)\n",
    "\n",
    "        loss += reg_loss\n",
    "\n",
    "        if (opt is not None)  :\n",
    "            with torch.no_grad():\n",
    "\n",
    "                opt.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                opt.step()\n",
    "\n",
    "        self.count+=1\n",
    "        if self.count  %3==0:\n",
    "            print(\"*\", end='')\n",
    "\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        del loss\n",
    "        del reg_loss\n",
    "\n",
    "        return loss_item, len(yb)#, acc\n",
    "\n",
    "    \n",
    "################################################################\n",
    "    def fit_dataloader_CLASS(self, dscrm_model,loader,   epochs = 1, validation_loader = None):\n",
    "        #_criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "        #_optimizer = optim.AdamW(self.parameters())\n",
    "        #_optimizer = optim.Adam(self.parameters(), lr = 0.00001)#, eps=0.0)\n",
    "        if (self._criterion is None): # or not isinstance(self._criterion, nn._Loss):\n",
    "            raise Exception(\"Loss-function is not select!\")\n",
    "\n",
    "        if (self._optimizer is None) or not isinstance(self._optimizer, optim.Optimizer):\n",
    "            raise Exception(\"Optimizer is not select!\")\n",
    "\n",
    "#        _criterion = nn.MSELoss(reduction='mean')\n",
    "#        _optimizer = optim.SGD(self.parameters(), lr=0.001, momentum=0.2)\n",
    "\n",
    "\n",
    "         \n",
    "            \n",
    "            \n",
    "        history = History()\n",
    "        self.count=0\n",
    "        for epoch in range(epochs):\n",
    "            self._optimizer.zero_grad()\n",
    "            \n",
    "            print(\"Epoch {0}/{1}\".format(epoch, epochs), end='')\n",
    "            \n",
    "            self.train()\n",
    "            ########################################3\n",
    "             \n",
    "            \n",
    "            ### train mode ###\n",
    "            print(\"[\", end='')\n",
    "            losses=[]\n",
    "            nums=[]\n",
    "            for s in loader:\n",
    "                \n",
    " \n",
    "                \n",
    "                train_ds = TensorDataset(                                                            \n",
    "                                        torch.FloatTensor(s['Anchor'].numpy()).to(self.device),\n",
    "                                        torch.FloatTensor(s['class_'].numpy()).to(self.device)  \n",
    "                                        )\n",
    "                \n",
    "                 \n",
    "                \n",
    "                 \n",
    "                \n",
    "                images_Anchor=train_ds.tensors[0] \n",
    "                 \n",
    "                class_=train_ds.tensors[1]\n",
    "                 \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                losses_, nums_   =   self.loss_batch_01(dscrm_model, \\\n",
    "                                                   (images_Anchor ,images_Anchor ),\\\n",
    "                                                    class_,  self._optimizer)                                                                                                       \n",
    "\n",
    "\n",
    "                losses.append(losses_)\n",
    "                nums.append(nums_ )\n",
    "                 \n",
    "                \n",
    "            print(\"]\", end='')\n",
    "\n",
    "\n",
    "            sum_nums = np.sum(nums)\n",
    "            loss = np.sum(np.multiply(losses, nums)) / sum_nums\n",
    "            ######################################\n",
    "             \n",
    "            ### test mode ###\n",
    "            if validation_loader is not None:\n",
    "                 \n",
    "\n",
    "\n",
    "                self.eval()\n",
    "                \n",
    "                 \n",
    "                print(\"[\", end='')\n",
    "                losses=[]\n",
    "                nums=[]\n",
    "                for s in validation_loader:\n",
    "                #s = next(iter(loader))\n",
    "                \n",
    "                    val_ds = TensorDataset(                                                            \n",
    "                                        torch.FloatTensor(s['Anchor'].numpy()).to(self.device),\n",
    "                                        torch.FloatTensor(s['class_'].numpy()).to(self.device)  \n",
    " \n",
    "                                        )\n",
    "                \n",
    "\n",
    "                    images_Anchor=val_ds.tensors[0] \n",
    "\n",
    "                    class_=val_ds.tensors[1]\n",
    "                     \n",
    "                \n",
    "                 \n",
    "                \n",
    "                     \n",
    "                \n",
    "                \n",
    "\n",
    "  \n",
    "                      \n",
    "                    \n",
    "                                                                                                                         \n",
    "\n",
    "                    losses_, nums_   =  \\\n",
    "                    self.loss_batch_01( dscrm_model,\\\n",
    "                           (images_Anchor ,images_Anchor ),\\\n",
    "                           class_, self._optimizer)      \n",
    "\n",
    "                    losses.append(losses_)\n",
    "                    nums.append(nums_ )\n",
    "                print(\"]\", end='')\n",
    "\n",
    "\n",
    "                sum_nums = np.sum(nums)\n",
    "                val_loss = np.sum(np.multiply(losses, nums)) / sum_nums\n",
    "                    #acc = np.sum(np.multiply(accs, nums)) / sum_nums\n",
    "                #################################################\n",
    "                history.add_epoch_values(epoch, {'loss': loss, 'val_loss': val_loss})\n",
    "                \n",
    "                print(' - Loss: {:.6f}'.format(loss), end='')\n",
    "                print(' - Test-loss: {:.6f}'.format(val_loss), end='')\n",
    "            else:\n",
    "                history.add_epoch_values(epoch, {'loss': loss })\n",
    "                print(' - Loss: {:.6f}'.format(loss), end='')\n",
    "                \n",
    "            print(\"\")\n",
    "            \n",
    " \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return history\n",
    "################################################################\n",
    "    def fit_dataloader_TL(self, dscrm_model,loader,   epochs = 1, validation_loader = None):\n",
    "        #_criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "        #_optimizer = optim.AdamW(self.parameters())\n",
    "        #_optimizer = optim.Adam(self.parameters(), lr = 0.00001)#, eps=0.0)\n",
    "        if (self._criterion is None): # or not isinstance(self._criterion, nn._Loss):\n",
    "            raise Exception(\"Loss-function is not select!\")\n",
    "\n",
    "        if (self._optimizer is None) or not isinstance(self._optimizer, optim.Optimizer):\n",
    "            raise Exception(\"Optimizer is not select!\")\n",
    "\n",
    "#        _criterion = nn.MSELoss(reduction='mean')\n",
    "#        _optimizer = optim.SGD(self.parameters(), lr=0.001, momentum=0.2)\n",
    "\n",
    "\n",
    "         \n",
    "            \n",
    "            \n",
    "        history = History()\n",
    "        self.count=0\n",
    "        for epoch in range(epochs):\n",
    "            self._optimizer.zero_grad()\n",
    "            \n",
    "            print(\"Epoch {0}/{1}\".format(epoch, epochs), end='')\n",
    "            \n",
    "            self.train()\n",
    "            ########################################3\n",
    "             \n",
    "            \n",
    "            ### train mode ###\n",
    "            print(\"[\", end='')\n",
    "            losses=[]\n",
    "            nums=[]\n",
    "            for s in loader:\n",
    "                \n",
    " \n",
    "                \n",
    "                train_ds = TensorDataset(                                                            \n",
    "                                        torch.FloatTensor(s['Anchor'].numpy()).to(self.device),\n",
    "                                        torch.FloatTensor(s['Positive'].numpy()).to(self.device) ,\n",
    "                                        torch.FloatTensor(s['Negative'].numpy()).to(self.device) \n",
    "                                        \n",
    "                                        )\n",
    "                \n",
    "                 \n",
    "                \n",
    "                 \n",
    "                \n",
    "                images_Anchor=train_ds.tensors[0] \n",
    "                 \n",
    "                images_Positive=train_ds.tensors[1]\n",
    "                images_Negative=train_ds.tensors[2]\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                losses_, nums_   =   self.loss_batch_02(dscrm_model, \\\n",
    "                                                   (images_Anchor ,images_Positive,images_Negative ),\\\n",
    "                                                      self._optimizer)                                                                                                       \n",
    "\n",
    "\n",
    "                losses.append(losses_)\n",
    "                nums.append(nums_ )\n",
    "                 \n",
    "                \n",
    "            print(\"]\", end='')\n",
    "\n",
    "\n",
    "            sum_nums = np.sum(nums)\n",
    "            loss = np.sum(np.multiply(losses, nums)) / sum_nums\n",
    "            ######################################\n",
    "             \n",
    "            ### test mode ###\n",
    "            if validation_loader is not None:\n",
    "                 \n",
    "\n",
    "\n",
    "                self.eval()\n",
    "                \n",
    "                 \n",
    "                print(\"[\", end='')\n",
    "                losses=[]\n",
    "                nums=[]\n",
    "                for s in validation_loader:\n",
    "                #s = next(iter(loader))\n",
    "                \n",
    "                    val_ds = TensorDataset(                                                            \n",
    "                                        torch.FloatTensor(s['Anchor'].numpy()).to(self.device),\n",
    "                                        torch.FloatTensor(s['Positive'].numpy()).to(self.device) ,\n",
    "                                        torch.FloatTensor(s['Negative'].numpy()).to(self.device) \n",
    "                                        \n",
    "                                        )\n",
    "                \n",
    "\n",
    "                    images_Anchor=val_ds.tensors[0] \n",
    "\n",
    "                    images_Positive=val_ds.tensors[1]\n",
    "                    images_Negative=val_ds.tensors[2]\n",
    "                \n",
    "                 \n",
    "                \n",
    "                     \n",
    "                \n",
    "                \n",
    "\n",
    "  \n",
    "                      \n",
    "                    \n",
    "                                                                                                                         \n",
    "\n",
    "                    losses_, nums_   =  \\\n",
    "                    self.loss_batch_00( dscrm_model,\\\n",
    "                           (images_Anchor ,images_Positive,images_Negative ),\\\n",
    "                            self._optimizer)      \n",
    "\n",
    "                    losses.append(losses_)\n",
    "                    nums.append(nums_ )\n",
    "                print(\"]\", end='')\n",
    "\n",
    "\n",
    "                sum_nums = np.sum(nums)\n",
    "                val_loss = np.sum(np.multiply(losses, nums)) / sum_nums\n",
    "                    #acc = np.sum(np.multiply(accs, nums)) / sum_nums\n",
    "                #################################################\n",
    "                history.add_epoch_values(epoch, {'loss': loss, 'val_loss': val_loss})\n",
    "                \n",
    "                print(' - Loss: {:.6f}'.format(loss), end='')\n",
    "                print(' - Test-loss: {:.6f}'.format(val_loss), end='')\n",
    "            else:\n",
    "                history.add_epoch_values(epoch, {'loss': loss })\n",
    "                print(' - Loss: {:.6f}'.format(loss), end='')\n",
    "                \n",
    "            print(\"\")\n",
    "            \n",
    " \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return history\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rental-jackson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 4, 32, 32]             104\n",
      "         LeakyReLU-2            [-1, 4, 32, 32]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           1,616\n",
      "         LeakyReLU-4             [-1, 16, 8, 8]               0\n",
      "            Conv2d-5             [-1, 22, 8, 8]           3,190\n",
      "         MaxPool2d-6             [-1, 22, 4, 4]               0\n",
      "         LeakyReLU-7             [-1, 22, 4, 4]               0\n",
      "           Flatten-8                  [-1, 352]               0\n",
      "conv_simple_features_00-9                  [-1, 352]               0\n",
      "           Linear-10                  [-1, 256]          90,368\n",
      "          Dropout-11                  [-1, 256]               0\n",
      "        LeakyReLU-12                  [-1, 256]               0\n",
      "      BatchNorm1d-13                  [-1, 256]             512\n",
      "           Linear-14                  [-1, 128]          32,896\n",
      "        LeakyReLU-15                  [-1, 128]               0\n",
      "           Linear-16                   [-1, 64]           8,256\n",
      "          Softmax-17                   [-1, 64]               0\n",
      "fully_connect_modul_264-18                   [-1, 64]               0\n",
      "   TL_002_mehanit-19                   [-1, 64]               0\n",
      "================================================================\n",
      "Total params: 136,942\n",
      "Trainable params: 136,942\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.52\n",
      "Estimated Total Size (MB): 0.63\n",
      "----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Loading preset weights... Ok.\n",
      "\n",
      "Loading preset weights... Fail! Error(s) in loading state_dict for fully_connect_modul_264:\n",
      "\tsize mismatch for D03.weight: copying a param with shape torch.Size([62, 128]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n",
      "\tsize mismatch for D03.bias: copying a param with shape torch.Size([62]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "[Action]: Reseting to random values!\n",
      "\n",
      "Loading preset weights... Ok.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = [128, 128, 1]   \n",
    "TL_001 = TL_002_mehanit(imageSize = IMAGE_SIZE, last_activate='linear', device='cpu',numclasses=64 ,show=0)\n",
    " \n",
    "TL_001.compile(criterion='000', optimizer='adam', lr=0.0011, momentum=0.5)\n",
    "TL_001.summary()\n",
    "#TL_001.load_state('TL_01.pt')\n",
    "TL_001.conv2Dfeatures.load_state('conv2Dfeatures01.pt')\n",
    "TL_001.fully_connect_modul_0.load_state('fully_connect_modul00.pt')         \n",
    "TL_001.fully_connect_modul_TL.load_state('fully_connect_modul01.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5461e5",
   "metadata": {},
   "source": [
    "# Смотрим что получаем из Модели с загруженными веами на выходе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dressed-miniature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 128, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = np.expand_dims(resize('test/1000.jpg'),0)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "legislative-scheme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0157, 0.0156, 0.0157, 0.0156, 0.0155, 0.0157, 0.0156, 0.0156, 0.0154,\n",
       "         0.0156, 0.0156, 0.0156, 0.0157, 0.0156, 0.0156, 0.0156, 0.0158, 0.0155,\n",
       "         0.0157, 0.0157, 0.0158, 0.0155, 0.0156, 0.0157, 0.0159, 0.0155, 0.0158,\n",
       "         0.0156, 0.0156, 0.0157, 0.0155, 0.0156, 0.0157, 0.0156, 0.0157, 0.0154,\n",
       "         0.0157, 0.0155, 0.0156, 0.0154, 0.0156, 0.0157, 0.0155, 0.0157, 0.0155,\n",
       "         0.0156, 0.0157, 0.0155, 0.0157, 0.0157, 0.0156, 0.0158, 0.0156, 0.0156,\n",
       "         0.0156, 0.0157, 0.0156, 0.0156, 0.0157, 0.0156, 0.0156, 0.0158, 0.0155,\n",
       "         0.0155]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TL_001.eval()\n",
    "TL_001.forward(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97249c9e",
   "metadata": {},
   "source": [
    "# Создаем тензор, главное чтобы сайзинг был корректный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fcbe267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 128, 128,1, requires_grad=True)\n",
    "dummy_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "attended-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#На вход подаем нашу модель. Тензор с нужным сайзингом. И как назовем onnx модель\n",
    "create_onnx(TL_001,dummy_input,\"resnet22.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "listed-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01572005 0.01556213 0.0157241  0.0156216  0.01549527 0.01571118\n",
      "  0.01564095 0.01556099 0.01544076 0.01563209 0.01562818 0.01560857\n",
      "  0.01568871 0.01561895 0.01563435 0.01559756 0.01577066 0.01549003\n",
      "  0.01565195 0.01570285 0.01577493 0.01553487 0.01561518 0.01565863\n",
      "  0.01585665 0.01550204 0.01579598 0.01557621 0.01557802 0.0157427\n",
      "  0.01548345 0.01559956 0.01570015 0.0156171  0.01568109 0.01539198\n",
      "  0.01569382 0.01549866 0.01562692 0.01544279 0.01561532 0.01573507\n",
      "  0.01552414 0.0156537  0.01554568 0.01561192 0.01572452 0.01551917\n",
      "  0.01567346 0.01569424 0.01559653 0.01576583 0.01559077 0.01558607\n",
      "  0.01562535 0.01570639 0.01563995 0.01556026 0.01568801 0.01558797\n",
      "  0.01564424 0.01578099 0.01552234 0.01553647]]\n"
     ]
    }
   ],
   "source": [
    "#На вход подаем название оннх модели и какую-то фотографию.\n",
    "load_onnx('resnet22.onnx','test/1000.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b4203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
