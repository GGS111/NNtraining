{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mat\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "from z_utils.utils_base_14 import *\n",
    "from z_utils.utils_7343_01 import *\n",
    "from onnx1 import *\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#import sys\n",
    "import os\n",
    "#import pathlib\n",
    "import time\n",
    "from z_utils.utils_base_14 import *\n",
    "from z_utils.utils_mat_torch_003 import *\n",
    "from z_utils.utils_7343_01 import *\n",
    "from zz.Model_SRR_deep_YUV import model_SRR_03_1cannal_skotch\n",
    "from zz.sketch2color_00 import N_sketch_2_color_00\n",
    " \n",
    "from skimage.morphology import skeletonize, thin \n",
    "from enum import Enum\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from zz.gan_struct_04a import Layer_09, quasy_conv_00,quasy_conv_01_soft, conv_layer_downsample_01,Layer_01\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Flatten\n",
    "from torch.nn import LeakyReLU\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Softmax  \n",
    "from torch.nn import MaxPool2d,AvgPool2d\n",
    "from enum import Enum\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import BatchNorm2d,BatchNorm1d\n",
    "from torch import nn\n",
    "#####################################################################3\n",
    "from zz.layers import Layer\n",
    "from zz.layers.Input import Input\n",
    "from zz.layers.Lambda import Lambda\n",
    "from zz.layers.Reshape import Reshape\n",
    "from zz.layers.Flatten import Flatten\n",
    "from zz.utils.torchsummary import summary as _summary\n",
    "from zz.utils.WrappedDataLoader import WrappedDataLoader\n",
    "from zz.utils.History import History\n",
    "from zz.utils.Regularizer import Regularizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from zz.Convolution_uno_01   import conv_layer_universal_uno_04,conv_layer_universal_uno_05\n",
    "from zz.layers.Layer_01 import Layer_01\n",
    "from zz.gan_struct_04a import Layer_06\n",
    "################################################3\n",
    "################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_simple_features_00(Layer_06):\n",
    "    def __init__(self,  device = None, L1 = 0., L2 = 0.,show=0):\n",
    "        super(conv_simple_features_00, self).__init__()\n",
    "        self.show = show\n",
    "        if (device is not None):\n",
    "            self.device = device if (not isinstance(device, str)) else torch.device(device)\n",
    "        else:\n",
    "            self.device = device if (device is not None) else \\\n",
    "                    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                \n",
    "        self.L1=L1\n",
    "        self.L2=L2\n",
    "        self.regularizer = Regularizer(L1, L2)\n",
    "        _layer_conv_31 = Conv2d(1,4, kernel_size=(5, 5),\n",
    "                            stride=(4, 4), padding = (2, 2), padding_mode = 'zeros', bias = True)\n",
    "        self.add_module('conv00', _layer_conv_31) \n",
    "        _layer_activation_1 = LeakyReLU(0.05) \n",
    "        self.add_module('activation_LeakyReLU', _layer_activation_1)\n",
    "        _layer_conv_32 = Conv2d(4,16, kernel_size=(5,5),\n",
    "                            stride=(4, 4), padding = (2, 2), padding_mode = 'zeros', bias = True)\n",
    "        \n",
    "        self.add_module('conv01', _layer_conv_32) \n",
    "        _layer_conv_33 = Conv2d(16,22, kernel_size=(3,3),\n",
    "                            stride=(1, 1), padding = (1, 1), padding_mode = 'zeros', bias = True)\n",
    "          \n",
    "        \n",
    "        self.add_module('conv02', _layer_conv_33) \n",
    "        _layer_pooling_1 = MaxPool2d(kernel_size=(2, 2))  \n",
    "        self.add_module('Pool_00', _layer_pooling_1) \n",
    "        self.add_module('fltn_1', Flatten( ))\n",
    "\n",
    "        self.to(self.device)\n",
    "        self.reset_parameters()\n",
    " \n",
    "    def forward(self, scatch0):\n",
    "        im_01_dwnsmpl=self.conv00(scatch0)\n",
    "        im_01_dwnsmpl=self.activation_LeakyReLU(im_01_dwnsmpl)\n",
    "        if self.show:\n",
    "            print('im_01_dwnsmpl',im_01_dwnsmpl.shape)    \n",
    "        im_02_dwnsmpl=self.conv01(im_01_dwnsmpl)\n",
    "        im_02_dwnsmpl=self.activation_LeakyReLU(im_02_dwnsmpl)\n",
    "        if self.show:\n",
    "            print('im_02_dwnsmpl',im_02_dwnsmpl.shape)    \n",
    "        im_03_dwnsmpl=self.conv02(im_02_dwnsmpl)\n",
    "        im_03_dwnsmpl=self.Pool_00(im_03_dwnsmpl) \n",
    "        im_03_dwnsmpl=self.activation_LeakyReLU(im_03_dwnsmpl)\n",
    "        if self.show:\n",
    "            print('im_03_dwnsmpl',im_03_dwnsmpl.shape)    \n",
    "        vect_00=self.fltn_1(im_03_dwnsmpl)\n",
    "        if self.show:\n",
    "            print('vect_00',vect_00.shape)   \n",
    "        return vect_00\n",
    "################################################3\n",
    "class fully_connect_modul_264(Layer_06):\n",
    "    def __init__(self,  device = None, L1 = 0., L2 = 0., numclasses=9, show=0):\n",
    "        super(fully_connect_modul_264, self).__init__()\n",
    "        self.show = show\n",
    "        if (device is not None):\n",
    "            self.device = device if (not isinstance(device, str)) else torch.device(device)\n",
    "        else:\n",
    "            self.device = device if (device is not None) else \\\n",
    "                    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.numclasses =numclasses        \n",
    "        self.L1=L1\n",
    "        self.L2=L2\n",
    "        self.regularizer = Regularizer(L1, L2)\n",
    "        _layer_activation_1 = LeakyReLU(0.05) \n",
    "        self.add_module('activation_LeakyReLU', _layer_activation_1)\n",
    "        _layer_D01 = Linear(352, 256, bias = True)\n",
    "        self.add_module('D01', _layer_D01)\n",
    "        _layer_Dropout01 = Dropout(0.5)\n",
    "        _layer_batch_norm_3 = BatchNorm1d(256)\n",
    "        self.add_module('Dropout01', _layer_Dropout01) \n",
    "        self.add_module('layer_batch_norm',  _layer_batch_norm_3) \n",
    "        _layer_D02 = Linear(256, 128, bias = True)\n",
    "        self.add_module('D02', _layer_D02)\n",
    "        _layer_D03 = Linear(128,  self.numclasses, bias = True)\n",
    "        self.add_module('D03', _layer_D03)\n",
    "       \n",
    "        \n",
    "        _layer_SfTMax = Softmax(dim = -1)\n",
    "        self.add_module('SfTMax', _layer_SfTMax)  \n",
    "        _layer_Sgmd = Sigmoid()\n",
    "        self.add_module('Sgmd', _layer_Sgmd)  \n",
    "        #########################\n",
    "        self.to(self.device)\n",
    "        self.reset_parameters()\n",
    " \n",
    "    def forward(self, vect_00):\n",
    "        vect_01=self.D01(vect_00)        \n",
    "        vect_01=self.Dropout01(vect_01)\n",
    "        vect_01=self.activation_LeakyReLU(vect_01)\n",
    "        if self.show:\n",
    "            print('vect_01',vect_01.shape) \n",
    "        vect_01=self.layer_batch_norm(vect_01)\n",
    "        if self.show:\n",
    "            print('vect_01 layer_batch_norm',vect_01.shape) \n",
    "        vect_02=self.D02(vect_01) \n",
    "        vect_02=self.activation_LeakyReLU(vect_02)\n",
    "        if self.show:\n",
    "            print('vect_02',vect_02.shape)    \n",
    "        vect_03=self.D03(vect_02) \n",
    "        vect_03=self.SfTMax(vect_03) \n",
    "         \n",
    "        if self.show:\n",
    "            print('vect_03',vect_03.shape)         \n",
    "        return vect_03\n",
    "################################################3\n",
    "class fully_connect_modul_265(Layer_06):\n",
    "    def __init__(self,  device = None, L1 = 0., L2 = 0.,   show=0):\n",
    "        super(fully_connect_modul_265, self).__init__()\n",
    "        self.show = show\n",
    "        if (device is not None):\n",
    "            self.device = device if (not isinstance(device, str)) else torch.device(device)\n",
    "        else:\n",
    "            self.device = device if (device is not None) else \\\n",
    "                    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "             \n",
    "        self.L1=L1\n",
    "        self.L2=L2\n",
    "        self.regularizer = Regularizer(L1, L2)\n",
    "        _layer_activation_1 = LeakyReLU(0.05) \n",
    "        self.add_module('activation_LeakyReLU', _layer_activation_1)\n",
    "        _layer_D01 = Linear(352, 256, bias = True)\n",
    "        self.add_module('D01', _layer_D01)\n",
    "        _layer_Dropout01 = Dropout(0.5)\n",
    "        _layer_batch_norm_3 = BatchNorm1d(256)\n",
    "        self.add_module('Dropout01', _layer_Dropout01) \n",
    "        self.add_module('layer_batch_norm',  _layer_batch_norm_3) \n",
    "        _layer_D02 = Linear(256, 128, bias = True)\n",
    "        self.add_module('D02', _layer_D02)\n",
    "        _layer_D03 = Linear(128,  64, bias = True)\n",
    "        self.add_module('D03', _layer_D03)\n",
    "       \n",
    "        \n",
    "        \n",
    "        _layer_Sgmd = Sigmoid()\n",
    "        self.add_module('Sgmd', _layer_Sgmd)  \n",
    "        #########################\n",
    "        self.to(self.device)\n",
    "        self.reset_parameters()\n",
    " \n",
    "    def forward(self, vect_00):\n",
    "        vect_01=self.D01(vect_00)        \n",
    "        vect_01=self.Dropout01(vect_01)\n",
    "        vect_01=self.activation_LeakyReLU(vect_01)\n",
    "        if self.show:\n",
    "            print('vect_01',vect_01.shape) \n",
    "        vect_01=self.layer_batch_norm(vect_01)\n",
    "        if self.show:\n",
    "            print('vect_01 layer_batch_norm',vect_01.shape) \n",
    "        vect_02=self.D02(vect_01) \n",
    "        vect_02=self.activation_LeakyReLU(vect_02)\n",
    "        if self.show:\n",
    "            print('vect_02',vect_02.shape)    \n",
    "        vect_03=self.D03(vect_02) \n",
    "        \n",
    "         \n",
    "        if self.show:\n",
    "            print('vect_03',vect_03.shape)         \n",
    "        return vect_03\n",
    "\n",
    "#№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№\n",
    "class TL_003_mehanit_onnx(Layer_06):\n",
    "    def __init__(self, imageSize,  last_activate, L1 = 0., L2 = 0.,device = None,numclasses=10,show=0 ):\n",
    "        super(TL_003_mehanit_onnx, self).__init__( (imageSize[0],imageSize[1],1)   )    \n",
    "\n",
    "        #self.class_name = str(self.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "        self.class_name = self.__class__.__name__\n",
    "        self.last_activate = last_activate\n",
    "        self.cannal_in= imageSize[2]\n",
    "         \n",
    "        self.imageSize = imageSize\n",
    "        self.regularizer = Regularizer(L1, L2)\n",
    "        self.show=show\n",
    "        self.L1=L1\n",
    "        self.L2=L2\n",
    "        self.numclasses=numclasses \n",
    "        self.criterion_tml = torch.nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "        if (device is not None):\n",
    "            self.device = device if (not isinstance(device, str)) else torch.device(device)\n",
    "        else:\n",
    "            self.device = device if (device is not None) else \\\n",
    "                    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        ##############3\n",
    "        self.conv2Dfeatures=conv_simple_features_00(device,L1 ,L2,show) \n",
    "        self.fully_connect_modul_0=fully_connect_modul_264(device,L1 ,L2,numclasses,show) \n",
    "        self.fully_connect_modul_TL=fully_connect_modul_265(device,L1 ,L2,show) \n",
    "         #######################\n",
    "        _layer_activation_1 = LeakyReLU(0.05) \n",
    "        self.add_module('activation_LeakyReLU', _layer_activation_1)\n",
    "        _layer_D01 = Linear(352, 256, bias = True)\n",
    "        self.add_module('D01', _layer_D01)\n",
    "        _layer_Dropout01 = Dropout(0.5)\n",
    "        _layer_batch_norm_3 = BatchNorm1d(256)\n",
    "        self.add_module('Dropout01', _layer_Dropout01) \n",
    "        self.add_module('layer_batch_norm',  _layer_batch_norm_3) \n",
    "        _layer_D02 = Linear(256, 128, bias = True)\n",
    "        self.add_module('D02', _layer_D02)\n",
    "        _layer_D03 = Linear(128,  self.numclasses, bias = True)\n",
    "        self.add_module('D03', _layer_D03)\n",
    "        #########################\n",
    "        \n",
    "        _layer_SfTMax = Softmax(dim = -1)\n",
    "        self.add_module('SfTMax', _layer_SfTMax)  \n",
    "        _layer_Sgmd = Sigmoid()\n",
    "        self.add_module('Sgmd', _layer_Sgmd)  \n",
    " \n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    #####################################################\n",
    "    def forward(self, scatch ):\n",
    "        class _type_input(Enum):\n",
    "            is_torch_tensor = 0\n",
    "            is_numpy = 1\n",
    "            is_list = 2\n",
    "        \n",
    "           \n",
    "        x_input = scatch\n",
    "        \n",
    "        _t_input = []\n",
    "        _x_input = []\n",
    "        for x in (x_input,x_input):\n",
    "            if isinstance(x, (torch.Tensor)):\n",
    "                _t_input.append(_type_input.is_torch_tensor)\n",
    "                _x_input.append(x)\n",
    "            elif isinstance(x, (np.ndarray)):\n",
    "                _t_input.append(_type_input.is_numpy)\n",
    "                _x_input.append(torch.FloatTensor(x).to(self.device))\n",
    "            elif isinstance(x, (list, tuple)):\n",
    "                _t_input.append(_type_input.is_list)\n",
    "                _x_input.append(torch.FloatTensor(x).to(self.device))\n",
    "            else:\n",
    "                raise Exception('Invalid type input')\n",
    "\n",
    "        _x_input = tuple(_x_input)\n",
    "        _t_input = tuple(_t_input)\n",
    "\n",
    "        scatch = self._contiguous(_x_input[0])\n",
    "      \n",
    "         \n",
    "         \n",
    "        ##############\n",
    "        \n",
    "            \n",
    "        \n",
    "        _layer_permut_channelfirst = Lambda(lambda x:  x.permute((0, 3, 1, 2)))\n",
    "        _layer_permut_channellast = Lambda(lambda x:  x.permute((0, 2, 3, 1)))\n",
    "\n",
    "        scatch0 = _layer_permut_channelfirst(scatch)\n",
    "        vect_00=self.conv2Dfeatures(scatch0)\n",
    "        vect_03=self.fully_connect_modul_TL(vect_00)    \n",
    "        ################################# \n",
    "         \n",
    "        ######################################\n",
    "        x = vect_03\n",
    "        x = self._contiguous(x)\n",
    "\n",
    "        ###################    \n",
    "\n",
    "        if _type_input.is_torch_tensor in _t_input:\n",
    "            pass\n",
    "        elif _type_input.is_numpy in _t_input:\n",
    "            if (self.device.type == \"cuda\"):\n",
    "                x = x.cpu().detach().numpy()\n",
    "            else:\n",
    "                x = x.detach().numpy()\n",
    "        else:\n",
    "            if (self.device.type == \"cuda\"):\n",
    "                x = x.cpu().detach().numpy().tolist()\n",
    "            else:\n",
    "                x = x.detach().numpy().tolist()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x \n",
    "     \n",
    "    def _get_regularizer(self):\n",
    "        return self.regularizer\n",
    "#################################################################\n",
    "###################################################################################################\n",
    "    def loss_batch_02(self,dsrmn_model,  x,   opt=None):\n",
    "#            def cross_entropy(pred, soft_targets):\n",
    "#                return -torch.log(torch.mean(torch.sum(soft_targets * pred, 1)))\n",
    "            #logsoftmax = nn.LogSoftmax()\n",
    "            #return torch.pow(1 - torch.mean(torch.sum(soft_targets * pred, 1)), 2)\n",
    "            #return torch.mean(torch.sum(- soft_targets * torch.norm(pred, p=2,dim=1), 1))\n",
    "            #return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n",
    "#            def l2_norm_diff(pred, soft_targets):\n",
    "#                return  torch.sqrt(torch.mean(torch.sum((soft_targets - pred )**2,-1)))\n",
    "            #logsoftmax = nn.LogSoftmax()\n",
    "            #return torch.pow(1 - torch.mean(torch.sum(soft_targets * pred, 1)), 2)\n",
    "            #return torch.mean(torch.sum(- soft_targets * torch.norm(pred, p=2,dim=1), 1))\n",
    "            #return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n",
    "        \n",
    "        xb=(x[0],x[0]) \n",
    "        pred_ancor_0,_ = self.forward_eshar_00(*xb)\n",
    "        xb=(x[1],x[1])\n",
    "        pred_positive_0,_ = self.forward_eshar_00(*xb)\n",
    "        xb=(x[2],x[2])\n",
    "        pred_neg_0,_ = self.forward_eshar_00(*xb)\n",
    "        #print('self._criterion',self._criterion)\n",
    "        loss_0=self.criterion_tml (pred_ancor_0,pred_positive_0,pred_neg_0)\n",
    "        loss_1=0#self.criterion_tml (pred_ancor_1,pred_positive_1,pred_neg_1)\n",
    "            \n",
    "            \n",
    "            \n",
    "             \n",
    "            \n",
    "             \n",
    "         \n",
    "        loss =1.0*loss_0+0.9*loss_1 \n",
    "        \n",
    "         \n",
    "        #print(' loss', loss) #,end='')    \n",
    "       \n",
    "        #print(self.loss_vgg_1_bw(pred0, yb),self._criterion(pred0, yb),self.MSELoss( dscrm_tenzor,0*dscrm_tenzor))\n",
    "         \n",
    "        \n",
    "        #loss+=  2.1*loss_mse \n",
    "        #loss = loss_func(pred0, yb)\n",
    "        #loss = cross_entropy(pred, yb)\n",
    "\n",
    "        #del(pred_ancor_0,pred_ancor_1,pred_positive_0,pred_positive_1,pred_neg_0,pred_neg_1) \n",
    "\n",
    "        #_, predicted = torch.max(pred.data, dim = 1)\n",
    "        #_, ind_target = torch.max(yb, dim = 1)\n",
    "        #correct = (predicted == ind_target).sum().item()\n",
    "        #acc = correct / len(yb) #.size(0)\n",
    "\n",
    "        _regularizer = self._get_regularizer()\n",
    "\n",
    "        reg_loss = 0\n",
    "        for param in self.parameters():\n",
    "            reg_loss += _regularizer(param)\n",
    "\n",
    "        loss += reg_loss\n",
    "\n",
    "        if (opt is not None) :\n",
    "            with torch.no_grad():\n",
    "\n",
    "                opt.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                opt.step()\n",
    "\n",
    "        self.count+=1\n",
    "        if self.count  %3==0:\n",
    "            print(\"*\", end='')\n",
    "\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        del loss\n",
    "        del reg_loss\n",
    "        \n",
    "        return loss_item, 1#, acc\n",
    "################################################################\n",
    "    #####################################################\n",
    "    def forward_eshar_00(self, scatch, im_wire):\n",
    "        # skotch_N_global, Ref_global,sketch_ref\n",
    "        class _type_input(Enum):\n",
    "            is_torch_tensor = 0\n",
    "            is_numpy = 1\n",
    "            is_list = 2\n",
    "        \n",
    "           \n",
    "        x_input = (scatch , im_wire)\n",
    "        \n",
    "        _t_input = []\n",
    "        _x_input = []\n",
    "        for x in x_input:\n",
    "            if isinstance(x, (torch.Tensor)):\n",
    "                _t_input.append(_type_input.is_torch_tensor)\n",
    "                _x_input.append(x)\n",
    "            elif isinstance(x, (np.ndarray)):\n",
    "                _t_input.append(_type_input.is_numpy)\n",
    "                _x_input.append(torch.FloatTensor(x).to(self.device))\n",
    "            elif isinstance(x, (list, tuple)):\n",
    "                _t_input.append(_type_input.is_list)\n",
    "                _x_input.append(torch.FloatTensor(x).to(self.device))\n",
    "            else:\n",
    "                raise Exception('Invalid type input')\n",
    "\n",
    "        _x_input = tuple(_x_input)\n",
    "        _t_input = tuple(_t_input)\n",
    "\n",
    "        scatch = self._contiguous(_x_input[0])\n",
    "        im_wire = self._contiguous(_x_input[1])\n",
    "         \n",
    "         \n",
    "        ##############\n",
    "        \n",
    "            \n",
    "        \n",
    "        _layer_permut_channelfirst = Lambda(lambda x:  x.permute((0, 3, 1, 2)))\n",
    "        _layer_permut_channellast = Lambda(lambda x:  x.permute((0, 2, 3, 1)))\n",
    "\n",
    "        scatch0 = _layer_permut_channelfirst(scatch)\n",
    "        vect_00=self.conv2Dfeatures(scatch0)\n",
    "        vect_03=self.fully_connect_modul_TL(vect_00)    \n",
    "        ################################# \n",
    "         \n",
    "        ######################################\n",
    "        x = vect_03\n",
    "        x = self._contiguous(x)\n",
    "\n",
    "        ###################    \n",
    "\n",
    "        if _type_input.is_torch_tensor in _t_input:\n",
    "            pass\n",
    "        elif _type_input.is_numpy in _t_input:\n",
    "            if (self.device.type == \"cuda\"):\n",
    "                x = x.cpu().detach().numpy()\n",
    "            else:\n",
    "                x = x.detach().numpy()\n",
    "        else:\n",
    "            if (self.device.type == \"cuda\"):\n",
    "                x = x.cpu().detach().numpy().tolist()\n",
    "            else:\n",
    "                x = x.detach().numpy().tolist()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x,0\n",
    "     \n",
    "    def _get_regularizer(self):\n",
    "        return self.regularizer\n",
    "\n",
    "###################################################################\n",
    "    def loss_batch_01(self,dsrmn_model, xb, yb,   opt=None):\n",
    "#            def cross_entropy(pred, soft_targets):\n",
    "#                return -torch.log(torch.mean(torch.sum(soft_targets * pred, 1)))\n",
    "            #logsoftmax = nn.LogSoftmax()\n",
    "            #return torch.pow(1 - torch.mean(torch.sum(soft_targets * pred, 1)), 2)\n",
    "            #return torch.mean(torch.sum(- soft_targets * torch.norm(pred, p=2,dim=1), 1))\n",
    "            #return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n",
    "#            def l2_norm_diff(pred, soft_targets):\n",
    "#                return  torch.sqrt(torch.mean(torch.sum((soft_targets - pred )**2,-1)))\n",
    "            #logsoftmax = nn.LogSoftmax()\n",
    "            #return torch.pow(1 - torch.mean(torch.sum(soft_targets * pred, 1)), 2)\n",
    "            #return torch.mean(torch.sum(- soft_targets * torch.norm(pred, p=2,dim=1), 1))\n",
    "            #return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n",
    "\n",
    "        #print(xb[0].shape)\n",
    "        pred = self(*xb)\n",
    "        #print(pred)  \n",
    "        Positive=xb[0][0].numpy() \n",
    "        #ai_2(Positive[:,:,0])\n",
    "        #print(yb)        \n",
    "        #print('999999999999999999999999999999999')    \n",
    "\n",
    "        if isinstance(pred, tuple):\n",
    "            pred0 = pred[0]\n",
    "            del pred\n",
    "        else:\n",
    "            pred0 = pred\n",
    "        loss=0\n",
    "        \n",
    "        #loss_mse= self._criterion(pred0, yb) \n",
    "        #print(pred0.shape)\n",
    "        #print(yb.shape)\n",
    "        MSELoss=nn.MSELoss(reduction='mean')    \n",
    "        loss_mse= MSELoss(pred0, yb) \n",
    "         \n",
    "        \n",
    "        loss +=1.1*loss_mse \n",
    "       \n",
    "        #print(self.loss_vgg_1_bw(pred0, yb),self._criterion(pred0, yb),self.MSELoss( dscrm_tenzor,0*dscrm_tenzor))\n",
    "         \n",
    "        \n",
    "        #loss+=  2.1*loss_mse \n",
    "        #loss = loss_func(pred0, yb)\n",
    "        #loss = cross_entropy(pred, yb)\n",
    "\n",
    "        del pred0\n",
    "\n",
    "        #_, predicted = torch.max(pred.data, dim = 1)\n",
    "        #_, ind_target = torch.max(yb, dim = 1)\n",
    "        #correct = (predicted == ind_target).sum().item()\n",
    "        #acc = correct / len(yb) #.size(0)\n",
    "\n",
    "        _regularizer = self._get_regularizer()\n",
    "\n",
    "        reg_loss = 0\n",
    "        for param in self.parameters():\n",
    "            reg_loss += _regularizer(param)\n",
    "\n",
    "        loss += reg_loss\n",
    "\n",
    "        if (opt is not None)  :\n",
    "            with torch.no_grad():\n",
    "\n",
    "                opt.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                opt.step()\n",
    "\n",
    "        self.count+=1\n",
    "        if self.count  %3==0:\n",
    "            print(\"*\", end='')\n",
    "\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        del loss\n",
    "        del reg_loss\n",
    "\n",
    "        return loss_item, len(yb)#, acc\n",
    "\n",
    "    \n",
    "################################################################\n",
    "    def fit_dataloader_CLASS(self, dscrm_model,loader,   epochs = 1, validation_loader = None):\n",
    "        #_criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "        #_optimizer = optim.AdamW(self.parameters())\n",
    "        #_optimizer = optim.Adam(self.parameters(), lr = 0.00001)#, eps=0.0)\n",
    "        if (self._criterion is None): # or not isinstance(self._criterion, nn._Loss):\n",
    "            raise Exception(\"Loss-function is not select!\")\n",
    "\n",
    "        if (self._optimizer is None) or not isinstance(self._optimizer, optim.Optimizer):\n",
    "            raise Exception(\"Optimizer is not select!\")\n",
    "\n",
    "#        _criterion = nn.MSELoss(reduction='mean')\n",
    "#        _optimizer = optim.SGD(self.parameters(), lr=0.001, momentum=0.2)\n",
    "\n",
    "\n",
    "         \n",
    "            \n",
    "            \n",
    "        history = History()\n",
    "        self.count=0\n",
    "        for epoch in range(epochs):\n",
    "            self._optimizer.zero_grad()\n",
    "            \n",
    "            print(\"Epoch {0}/{1}\".format(epoch, epochs), end='')\n",
    "            \n",
    "            self.train()\n",
    "            ########################################3\n",
    "             \n",
    "            \n",
    "            ### train mode ###\n",
    "            print(\"[\", end='')\n",
    "            losses=[]\n",
    "            nums=[]\n",
    "            for s in loader:\n",
    "                \n",
    " \n",
    "                \n",
    "                train_ds = TensorDataset(                                                            \n",
    "                                        torch.FloatTensor(s['Anchor'].numpy()).to(self.device),\n",
    "                                        torch.FloatTensor(s['class_'].numpy()).to(self.device)  \n",
    "                                        )\n",
    "                \n",
    "                 \n",
    "                \n",
    "                 \n",
    "                \n",
    "                images_Anchor=train_ds.tensors[0] \n",
    "                 \n",
    "                class_=train_ds.tensors[1]\n",
    "                 \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                losses_, nums_   =   self.loss_batch_01(dscrm_model, \\\n",
    "                                                   (images_Anchor ,images_Anchor ),\\\n",
    "                                                    class_,  self._optimizer)                                                                                                       \n",
    "\n",
    "\n",
    "                losses.append(losses_)\n",
    "                nums.append(nums_ )\n",
    "                 \n",
    "                \n",
    "            print(\"]\", end='')\n",
    "\n",
    "\n",
    "            sum_nums = np.sum(nums)\n",
    "            loss = np.sum(np.multiply(losses, nums)) / sum_nums\n",
    "            ######################################\n",
    "             \n",
    "            ### test mode ###\n",
    "            if validation_loader is not None:\n",
    "                 \n",
    "\n",
    "\n",
    "                self.eval()\n",
    "                \n",
    "                 \n",
    "                print(\"[\", end='')\n",
    "                losses=[]\n",
    "                nums=[]\n",
    "                for s in validation_loader:\n",
    "                #s = next(iter(loader))\n",
    "                \n",
    "                    val_ds = TensorDataset(                                                            \n",
    "                                        torch.FloatTensor(s['Anchor'].numpy()).to(self.device),\n",
    "                                        torch.FloatTensor(s['class_'].numpy()).to(self.device)  \n",
    " \n",
    "                                        )\n",
    "                \n",
    "\n",
    "                    images_Anchor=val_ds.tensors[0] \n",
    "\n",
    "                    class_=val_ds.tensors[1]\n",
    "                     \n",
    "                \n",
    "                 \n",
    "                \n",
    "                     \n",
    "                \n",
    "                \n",
    "\n",
    "  \n",
    "                      \n",
    "                    \n",
    "                                                                                                                         \n",
    "\n",
    "                    losses_, nums_   =  \\\n",
    "                    self.loss_batch_01( dscrm_model,\\\n",
    "                           (images_Anchor ,images_Anchor ),\\\n",
    "                           class_, self._optimizer)      \n",
    "\n",
    "                    losses.append(losses_)\n",
    "                    nums.append(nums_ )\n",
    "                print(\"]\", end='')\n",
    "\n",
    "\n",
    "                sum_nums = np.sum(nums)\n",
    "                val_loss = np.sum(np.multiply(losses, nums)) / sum_nums\n",
    "                    #acc = np.sum(np.multiply(accs, nums)) / sum_nums\n",
    "                #################################################\n",
    "                history.add_epoch_values(epoch, {'loss': loss, 'val_loss': val_loss})\n",
    "                \n",
    "                print(' - Loss: {:.6f}'.format(loss), end='')\n",
    "                print(' - Test-loss: {:.6f}'.format(val_loss), end='')\n",
    "            else:\n",
    "                history.add_epoch_values(epoch, {'loss': loss })\n",
    "                print(' - Loss: {:.6f}'.format(loss), end='')\n",
    "                \n",
    "            print(\"\")\n",
    "            \n",
    " \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return history\n",
    "################################################################\n",
    "    def fit_dataloader_TL(self, dscrm_model,loader,   epochs = 1, validation_loader = None):\n",
    "        #_criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "        #_optimizer = optim.AdamW(self.parameters())\n",
    "        #_optimizer = optim.Adam(self.parameters(), lr = 0.00001)#, eps=0.0)\n",
    "        if (self._criterion is None): # or not isinstance(self._criterion, nn._Loss):\n",
    "            raise Exception(\"Loss-function is not select!\")\n",
    "\n",
    "        if (self._optimizer is None) or not isinstance(self._optimizer, optim.Optimizer):\n",
    "            raise Exception(\"Optimizer is not select!\")\n",
    "\n",
    "#        _criterion = nn.MSELoss(reduction='mean')\n",
    "#        _optimizer = optim.SGD(self.parameters(), lr=0.001, momentum=0.2)\n",
    "\n",
    "\n",
    "         \n",
    "            \n",
    "            \n",
    "        history = History()\n",
    "        self.count=0\n",
    "        for epoch in range(epochs):\n",
    "            self._optimizer.zero_grad()\n",
    "            \n",
    "            print(\"Epoch {0}/{1}\".format(epoch, epochs), end='')\n",
    "            \n",
    "            self.train()\n",
    "            ########################################3\n",
    "             \n",
    "            \n",
    "            ### train mode ###\n",
    "            print(\"[\", end='')\n",
    "            losses=[]\n",
    "            nums=[]\n",
    "            for s in loader:\n",
    "                \n",
    " \n",
    "                \n",
    "                train_ds = TensorDataset(                                                            \n",
    "                                        torch.FloatTensor(s['Anchor'].numpy()).to(self.device),\n",
    "                                        torch.FloatTensor(s['Positive'].numpy()).to(self.device) ,\n",
    "                                        torch.FloatTensor(s['Negative'].numpy()).to(self.device) \n",
    "                                        \n",
    "                                        )\n",
    "                \n",
    "                 \n",
    "                \n",
    "                 \n",
    "                \n",
    "                images_Anchor=train_ds.tensors[0] \n",
    "                 \n",
    "                images_Positive=train_ds.tensors[1]\n",
    "                images_Negative=train_ds.tensors[2]\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                losses_, nums_   =   self.loss_batch_02(dscrm_model, \\\n",
    "                                                   (images_Anchor ,images_Positive,images_Negative ),\\\n",
    "                                                      self._optimizer)                                                                                                       \n",
    "\n",
    "\n",
    "                losses.append(losses_)\n",
    "                nums.append(nums_ )\n",
    "                 \n",
    "                \n",
    "            print(\"]\", end='')\n",
    "\n",
    "\n",
    "            sum_nums = np.sum(nums)\n",
    "            loss = np.sum(np.multiply(losses, nums)) / sum_nums\n",
    "            ######################################\n",
    "             \n",
    "            ### test mode ###\n",
    "            if validation_loader is not None:\n",
    "                 \n",
    "\n",
    "\n",
    "                self.eval()\n",
    "                \n",
    "                 \n",
    "                print(\"[\", end='')\n",
    "                losses=[]\n",
    "                nums=[]\n",
    "                for s in validation_loader:\n",
    "                #s = next(iter(loader))\n",
    "                \n",
    "                    val_ds = TensorDataset(                                                            \n",
    "                                        torch.FloatTensor(s['Anchor'].numpy()).to(self.device),\n",
    "                                        torch.FloatTensor(s['Positive'].numpy()).to(self.device) ,\n",
    "                                        torch.FloatTensor(s['Negative'].numpy()).to(self.device) \n",
    "                                        \n",
    "                                        )\n",
    "                \n",
    "\n",
    "                    images_Anchor=val_ds.tensors[0] \n",
    "\n",
    "                    images_Positive=val_ds.tensors[1]\n",
    "                    images_Negative=val_ds.tensors[2]\n",
    "                \n",
    "                 \n",
    "                \n",
    "                     \n",
    "                \n",
    "                \n",
    "\n",
    "  \n",
    "                      \n",
    "                    \n",
    "                                                                                                                         \n",
    "\n",
    "                    losses_, nums_   =  \\\n",
    "                    self.loss_batch_00( dscrm_model,\\\n",
    "                           (images_Anchor ,images_Positive,images_Negative ),\\\n",
    "                            self._optimizer)      \n",
    "\n",
    "                    losses.append(losses_)\n",
    "                    nums.append(nums_ )\n",
    "                print(\"]\", end='')\n",
    "\n",
    "\n",
    "                sum_nums = np.sum(nums)\n",
    "                val_loss = np.sum(np.multiply(losses, nums)) / sum_nums\n",
    "                    #acc = np.sum(np.multiply(accs, nums)) / sum_nums\n",
    "                #################################################\n",
    "                history.add_epoch_values(epoch, {'loss': loss, 'val_loss': val_loss})\n",
    "                \n",
    "                print(' - Loss: {:.6f}'.format(loss), end='')\n",
    "                print(' - Test-loss: {:.6f}'.format(val_loss), end='')\n",
    "            else:\n",
    "                history.add_epoch_values(epoch, {'loss': loss })\n",
    "                print(' - Loss: {:.6f}'.format(loss), end='')\n",
    "                \n",
    "            print(\"\")\n",
    "            \n",
    " \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading preset weights... Ok.\n",
      "\n",
      "Loading preset weights... Ok.\n",
      "\n",
      "Loading preset weights... Ok.\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "#from DiffuseMap_03b import *\n",
    "IMAGE_SIZE = [128, 128, 1] \n",
    "TL_001 = TL_003_mehanit_onnx(imageSize = IMAGE_SIZE, last_activate='linear', device='cpu',numclasses=670 ,show=0)\n",
    "\n",
    "TL_001.conv2Dfeatures.load_state('conv2Dfeatures04.pt')\n",
    "TL_001.fully_connect_modul_0.load_state('fully_connect_modul04.pt')         \n",
    "TL_001.fully_connect_modul_TL.load_state('fully_connect_modul_TL_04.pt')         \n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 128, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = np.expand_dims(resize('test/1000.jpg'),0)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83378935, -1.7313656 ,  0.97099924,  0.35577345, -0.8123244 ,\n",
       "         0.7427624 , -0.1118753 ,  1.419706  ,  0.33110735,  2.111747  ,\n",
       "         1.7316674 ,  0.6311197 , -1.4470496 , -0.4124527 , -1.3304124 ,\n",
       "         0.157992  ,  2.1160939 , -1.0186777 , -0.00499617,  0.5254518 ,\n",
       "        -0.08201496,  1.174205  , -0.08335625, -0.45413488,  1.3502662 ,\n",
       "         2.00027   , -0.6860363 , -1.2502449 , -0.77918273,  0.10626437,\n",
       "        -0.17604929, -0.28023157,  0.78639936, -0.96663314, -0.25876153,\n",
       "         1.0345687 ,  0.90125334, -0.12163695,  0.29197535, -1.187147  ,\n",
       "         0.42804784, -0.11927428, -0.4903878 , -0.19574943, -1.3841686 ,\n",
       "         2.2772772 , -0.04237171,  3.2458415 ,  0.4101629 ,  1.1224278 ,\n",
       "        -0.7329112 ,  0.85389763,  1.0796156 ,  0.41011453, -0.48651248,\n",
       "         0.35534945,  1.0595276 ,  1.7762328 , -0.6484624 , -0.17006455,\n",
       "         0.35265434,  0.90103495,  2.2325475 , -1.0753523 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TL_001.eval()\n",
    "TL_001.forward(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 128, 128,1, requires_grad=True)\n",
    "create_onnx(TL_001,dummy_input,\"TL_001.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83378893 -1.7313658   0.97099847  0.3557729  -0.8123252   0.7427623\n",
      "  -0.11187487  1.4197063   0.33110765  2.1117475   1.7316678   0.63112\n",
      "  -1.4470497  -0.4124534  -1.3304131   0.15799233  2.1160944  -1.0186781\n",
      "  -0.00499643  0.5254519  -0.08201527  1.174205   -0.08335635 -0.45413473\n",
      "   1.3502661   2.0002701  -0.6860362  -1.250245   -0.77918315  0.10626456\n",
      "  -0.17604968 -0.28023192  0.78639966 -0.9666332  -0.2587619   1.0345687\n",
      "   0.90125364 -0.12163719  0.2919758  -1.1871469   0.4280479  -0.11927456\n",
      "  -0.4903877  -0.19574949 -1.3841695   2.2772768  -0.04237161  3.2458405\n",
      "   0.41016302  1.1224284  -0.7329112   0.853898    1.0796154   0.41011465\n",
      "  -0.48651195  0.35534927  1.0595276   1.7762334  -0.64846224 -0.1700647\n",
      "   0.35265505  0.9010354   2.2325473  -1.0753517 ]]\n"
     ]
    }
   ],
   "source": [
    "load_onnx('TL_001.onnx','test/1000.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
